{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport requests\n\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence, pad_packed_sequence\n\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer, ngrams_iterator","metadata":{"execution":{"iopub.status.busy":"2023-12-24T06:14:39.480072Z","iopub.execute_input":"2023-12-24T06:14:39.481040Z","iopub.status.idle":"2023-12-24T06:14:39.488122Z","shell.execute_reply.started":"2023-12-24T06:14:39.480987Z","shell.execute_reply":"2023-12-24T06:14:39.486643Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"file_url = 'https://raw.githubusercontent.com/google-research/google-research/master/mbpp/mbpp.jsonl'\nresponse = requests.get(file_url)\n\ndata = list()\n\nif response.status_code == 200:\n    json_lines = response.text.splitlines()\n    for line in json_lines:\n        json_object = json.loads(line)\n        data.append(json_object)\nelse:\n    print(f\"Failed to download file. Status code: {response.status_code}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key, values in data[0].items():\n    print(f\"{key} - {json_object[key]}\")\n\n    print(\"*\" * 10)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T05:18:08.904057Z","iopub.execute_input":"2023-12-24T05:18:08.904754Z","iopub.status.idle":"2023-12-24T05:18:08.909661Z","shell.execute_reply.started":"2023-12-24T05:18:08.904718Z","shell.execute_reply":"2023-12-24T05:18:08.908759Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"text - Write a function to find the minimum total path sum in the given triangle.\n**********\ncode - def min_sum_path(A): \n\tmemo = [None] * len(A) \n\tn = len(A) - 1\n\tfor i in range(len(A[n])): \n\t\tmemo[i] = A[n][i] \n\tfor i in range(len(A) - 2, -1,-1): \n\t\tfor j in range( len(A[i])): \n\t\t\tmemo[j] = A[i][j] + min(memo[j], \n\t\t\t\t\t\t\t\t\tmemo[j + 1]) \n\treturn memo[0]\n**********\ntask_id - 974\n**********\ntest_setup_code - \n**********\ntest_list - ['assert min_sum_path([[ 2 ], [3, 9 ], [1, 6, 7 ]]) == 6', 'assert min_sum_path([[ 2 ], [3, 7 ], [8, 5, 6 ]]) == 10 ', 'assert min_sum_path([[ 3 ], [6, 4 ], [5, 2, 7 ]]) == 9']\n**********\nchallenge_test_list - []\n**********\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.DataFrame(data)\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T05:30:11.401407Z","iopub.execute_input":"2023-12-24T05:30:11.402391Z","iopub.status.idle":"2023-12-24T05:30:11.420449Z","shell.execute_reply.started":"2023-12-24T05:30:11.402352Z","shell.execute_reply":"2023-12-24T05:30:11.419509Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  Write a function to find the minimum cost path...   \n1  Write a function to find the similar elements ...   \n2  Write a python function to identify non-prime ...   \n3  Write a function to find the largest integers ...   \n4  Write a function to find the number of ways to...   \n\n                                                code  task_id test_setup_code  \\\n0  R = 3\\r\\nC = 3\\r\\ndef min_cost(cost, m, n): \\r...        1                   \n1  def similar_elements(test_tup1, test_tup2):\\r\\...        2                   \n2  import math\\r\\ndef is_not_prime(n):\\r\\n    res...        3                   \n3  import heapq as hq\\r\\ndef heap_queue_largest(n...        4                   \n4  def count_ways(n): \\r\\n\\tA = [0] * (n + 1) \\r\\...        5                   \n\n                                           test_list challenge_test_list  \n0  [assert min_cost([[1, 2, 3], [4, 8, 2], [1, 5,...                  []  \n1  [assert similar_elements((3, 4, 5, 6),(5, 7, 4...                  []  \n2  [assert is_not_prime(2) == False, assert is_no...                  []  \n3  [assert heap_queue_largest( [25, 35, 22, 85, 1...                  []  \n4  [assert count_ways(2) == 3, assert count_ways(...                  []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>code</th>\n      <th>task_id</th>\n      <th>test_setup_code</th>\n      <th>test_list</th>\n      <th>challenge_test_list</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Write a function to find the minimum cost path...</td>\n      <td>R = 3\\r\\nC = 3\\r\\ndef min_cost(cost, m, n): \\r...</td>\n      <td>1</td>\n      <td></td>\n      <td>[assert min_cost([[1, 2, 3], [4, 8, 2], [1, 5,...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Write a function to find the similar elements ...</td>\n      <td>def similar_elements(test_tup1, test_tup2):\\r\\...</td>\n      <td>2</td>\n      <td></td>\n      <td>[assert similar_elements((3, 4, 5, 6),(5, 7, 4...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Write a python function to identify non-prime ...</td>\n      <td>import math\\r\\ndef is_not_prime(n):\\r\\n    res...</td>\n      <td>3</td>\n      <td></td>\n      <td>[assert is_not_prime(2) == False, assert is_no...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Write a function to find the largest integers ...</td>\n      <td>import heapq as hq\\r\\ndef heap_queue_largest(n...</td>\n      <td>4</td>\n      <td></td>\n      <td>[assert heap_queue_largest( [25, 35, 22, 85, 1...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Write a function to find the number of ways to...</td>\n      <td>def count_ways(n): \\r\\n\\tA = [0] * (n + 1) \\r\\...</td>\n      <td>5</td>\n      <td></td>\n      <td>[assert count_ways(2) == 3, assert count_ways(...</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample_data = df.sample(1)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T05:46:04.935939Z","iopub.execute_input":"2023-12-24T05:46:04.936994Z","iopub.status.idle":"2023-12-24T05:46:04.942175Z","shell.execute_reply.started":"2023-12-24T05:46:04.936955Z","shell.execute_reply":"2023-12-24T05:46:04.940996Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"tokenizer = get_tokenizer('basic_english')","metadata":{"execution":{"iopub.status.busy":"2023-12-24T05:46:04.982174Z","iopub.execute_input":"2023-12-24T05:46:04.982470Z","iopub.status.idle":"2023-12-24T05:46:04.988940Z","shell.execute_reply.started":"2023-12-24T05:46:04.982444Z","shell.execute_reply":"2023-12-24T05:46:04.988105Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"\" \".join([text.split() for text in sample_data['text']][0])","metadata":{"execution":{"iopub.status.busy":"2023-12-24T05:48:51.339327Z","iopub.execute_input":"2023-12-24T05:48:51.340095Z","iopub.status.idle":"2023-12-24T05:48:51.346365Z","shell.execute_reply.started":"2023-12-24T05:48:51.340052Z","shell.execute_reply":"2023-12-24T05:48:51.345492Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"\"Write a function to find all words starting with 'a' or 'e' in a given string.\""},"metadata":{}}]},{"cell_type":"code","source":"\" \".join([tokenizer(text) for text in sample_data['text']][0])","metadata":{"execution":{"iopub.status.busy":"2023-12-24T05:49:14.995195Z","iopub.execute_input":"2023-12-24T05:49:14.995576Z","iopub.status.idle":"2023-12-24T05:49:15.002021Z","shell.execute_reply.started":"2023-12-24T05:49:14.995545Z","shell.execute_reply":"2023-12-24T05:49:15.001152Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"\"write a function to find all words starting with ' a ' or ' e ' in a given string .\""},"metadata":{}}]},{"cell_type":"code","source":"class Preprocessor():\n    def __init__(self, df, tokenize=True):\n        self.df = df\n        self.tokenize_ = tokenize\n        self.text_tokenized = None\n        self.code_tokenized = None\n        self.text_vocab = None\n        self.code_vocab = None\n        self.text_numericalized = None\n        self.code_numericalized = None\n        self.text_padded = None\n        self.code_padded = None\n    \n    def functionToTokenize(self):\n        if self.tokenize_:\n            tokenizer = get_tokenizer('basic_english')\n            self.text_tokenized = [tokenizer(text) for text in self.df['text']]\n            self.code_tokenized = [tokenizer(code) for code in self.df['code']]\n        else:\n            self.text_tokenized = [text.split() for text in self.df['text']]\n            self.code_tokenized = [code.split() for code in self.df['code']]\n            \n    def functionToVocab(self):\n        self.text_vocab = build_vocab_from_iterator(self.text_tokenized, specials=['<pad>'])\n        self.code_vocab = build_vocab_from_iterator(self.code_tokenized, specials=['<pad>'])\n\n        self.text_vocab.set_default_index(self.text_vocab['<pad>'])\n        self.code_vocab.set_default_index(self.code_vocab['<pad>'])\n\n        self.text_numericalized = [self.text_vocab.lookup_indices(tokens) for tokens in self.text_tokenized]\n        self.code_numericalized = [self.code_vocab.lookup_indices(tokens) for tokens in self.code_tokenized]\n\n    def functionToPad(self):\n        self.text_padded = pad_sequence([torch.tensor(indices) for indices in self.text_numericalized], padding_value=self.text_vocab['<pad>'])\n        self.code_padded = pad_sequence([torch.tensor(indices) for indices in self.code_numericalized], padding_value=self.code_vocab['<pad>'])\n        \n        return self.text_padded, self.code_padded\n    \n    def setup(self):\n        self.functionToTokenize()\n        self.functionToVocab()\n        return self.functionToPad()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T06:08:48.068377Z","iopub.execute_input":"2023-12-24T06:08:48.068920Z","iopub.status.idle":"2023-12-24T06:08:48.081783Z","shell.execute_reply.started":"2023-12-24T06:08:48.068877Z","shell.execute_reply":"2023-12-24T06:08:48.080714Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"preprocessor = Preprocessor(df)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T06:08:48.293893Z","iopub.execute_input":"2023-12-24T06:08:48.294212Z","iopub.status.idle":"2023-12-24T06:08:48.298505Z","shell.execute_reply.started":"2023-12-24T06:08:48.294185Z","shell.execute_reply":"2023-12-24T06:08:48.297542Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"text_padded, code_padded = preprocessor.setup()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T06:09:30.149938Z","iopub.execute_input":"2023-12-24T06:09:30.150334Z","iopub.status.idle":"2023-12-24T06:09:30.427195Z","shell.execute_reply.started":"2023-12-24T06:09:30.150304Z","shell.execute_reply":"2023-12-24T06:09:30.426326Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"len(text_padded), len(code_padded)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T06:09:30.429057Z","iopub.execute_input":"2023-12-24T06:09:30.429677Z","iopub.status.idle":"2023-12-24T06:09:30.435903Z","shell.execute_reply.started":"2023-12-24T06:09:30.429620Z","shell.execute_reply":"2023-12-24T06:09:30.435033Z"},"trusted":true},"execution_count":130,"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"(48, 258)"},"metadata":{}}]},{"cell_type":"code","source":"class CodeGenerationDataset(Dataset):\n    def __init__(self, text_padded, code_padded):\n        self.text_padded = text_padded\n        self.code_padded = code_padded\n\n    def __len__(self):\n        return len(self.text_padded)\n\n    def __getitem__(self, idx):\n        return {'text': self.text_padded[idx], 'code': self.code_padded[idx]}","metadata":{"execution":{"iopub.status.busy":"2023-12-24T06:10:17.811104Z","iopub.execute_input":"2023-12-24T06:10:17.811897Z","iopub.status.idle":"2023-12-24T06:10:17.817780Z","shell.execute_reply.started":"2023-12-24T06:10:17.811858Z","shell.execute_reply":"2023-12-24T06:10:17.816822Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"dataset = CodeGenerationDataset(text_padded, code_padded)\nbatch_size = 32\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T06:10:30.867230Z","iopub.execute_input":"2023-12-24T06:10:30.867917Z","iopub.status.idle":"2023-12-24T06:10:30.873147Z","shell.execute_reply.started":"2023-12-24T06:10:30.867882Z","shell.execute_reply":"2023-12-24T06:10:30.872186Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"for batch in dataloader:\n    texts = batch['text']\n    codes = batch['code']\n    print(texts)\n    print()\n    print(codes)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-12-24T06:11:00.923241Z","iopub.execute_input":"2023-12-24T06:11:00.923910Z","iopub.status.idle":"2023-12-24T06:11:00.939253Z","shell.execute_reply.started":"2023-12-24T06:11:00.923872Z","shell.execute_reply":"2023-12-24T06:11:00.938332Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stdout","text":"tensor([[  0,   0,   0,  ...,   0,   0,   0],\n        [426,   0,   0,  ...,   0,   0,   0],\n        [212,   0,   0,  ...,   0,   0,   0],\n        ...,\n        [  0,   0,   0,  ...,   0,   0,   0],\n        [ 49,   0,   0,  ...,   0,   0,   0],\n        [425,  24,  17,  ...,  26,   6, 171]])\n\ntensor([[2915,    0,    0,  ...,    0,    0,   57],\n        [  10,    0,   52,  ...,    0,    0,   17],\n        [  10,    0,   16,  ...,    0,    0,   26],\n        ...,\n        [ 321,    0,    0,  ...,    0,    0,   23],\n        [  34,    0,   18,  ...,    0,    0, 1573],\n        [2378,   21,   27,  ...,   21,  588,  888]])\n","output_type":"stream"}]},{"cell_type":"code","source":"class Seq2SeqModel(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, pad_idx):\n        super(Seq2SeqModel, self).__init__()\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.GRU(embedding_size, hidden_size)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.pad_idx = pad_idx\n\n    def forward(self, input_sequence):\n        embedded = self.embedding(input_sequence)\n        output, hidden = self.rnn(embedded)\n        return self.fc(output)\n\n# Example usage:\n# Assuming text_vocab_size and code_vocab_size are the sizes of your vocabularies\ntext_vocab_size = len(preprocessor.text_vocab)\ncode_vocab_size = len(preprocessor.code_vocab)\nembedding_size = 256\nhidden_size = 512\noutput_size = code_vocab_size  # Output size is the size of the code vocabulary\npad_idx = preprocessor.text_vocab['<pad>']  # Use the pad index from the text vocabulary\n\n# Initialize the model\nmodel = Seq2SeqModel(text_vocab_size, embedding_size, hidden_size, output_size, pad_idx)\n\n# Example training loop (you will need to customize this based on your task)\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Move the model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Move the preprocessed data to GPU if available\ntext_padded = text_padded.to(device)\ncode_padded = code_padded.to(device)\n\n# Training loop\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n\n    output = model(text_padded)\n\n    # Calculate loss\n    loss = criterion(output.view(-1, output.size(2)), code_padded.view(-1))\n\n    # Backward pass and optimization\n    loss.backward()\n    optimizer.step()\n\n    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')","metadata":{"execution":{"iopub.status.busy":"2023-12-24T06:14:42.016848Z","iopub.execute_input":"2023-12-24T06:14:42.017613Z","iopub.status.idle":"2023-12-24T06:14:48.252367Z","shell.execute_reply.started":"2023-12-24T06:14:42.017572Z","shell.execute_reply":"2023-12-24T06:14:48.251074Z"},"trusted":true},"execution_count":137,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[137], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m model(text_padded)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_padded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     51\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (46752) to match target batch_size (251292)."],"ename":"ValueError","evalue":"Expected input batch_size (46752) to match target batch_size (251292).","output_type":"error"}]}]}